---
layout: post
title: HBase使用笔记
description: HBase经历多年打磨，应用实践，大势已成，已经得到广泛的应用，不阔以不学习了解下。
category: blog
---

##黎明

HBase列式存储数据库以列为单位聚合数据，将列值顺序地存入磁盘。这样设计基于一个假设：对特定的查询,不是所有的值都是必须的。同时还有可以便于压缩的优点。

下面是一个保存点击信息的SQL表设计:

	User
	id	username	credentials	roles	firstname	lastname	email
	
	ShortURL
	userId urlId shortId refShortId 
	
	URL
	id url refShortId title description content
	
	CLICK
	id datestamp shortId category dimension counter
	
短网址存储于ShortUrl表中，用户点击短网址来连接至完整网址。用户点击的信息记录在CLICK表中，用户信息存储在User中，网页信息存储在URL中。

对应的存储在列模式中的情形则是:

	

HBase的存储结构可以用下面数据结构来理解:

	SortedMap<RowKey, List<SortedMap<Column, List<Value, Timestamp>>>>
	
	
	
1.连接HBase中的表testtable,用户名：root,密码：root

    public void ConnectHBaseTable()
    {
        Configuration conf = new Configuration();       
        conf.set("hadoop.job.ugi", "root,root");      
        HBaseConfiguration config = new HBaseConfiguration();
        try
        {
            table = new HTable(config, "testtable");
        }catch(Exception e){e.printStackTrace();}
    }



2.根据行名name获得一行数据，存入Result.注意HBase中的表数据是字节存储的。
下面的例子表示获得行名为name的行的famA列族col1列的数据。


      String rowId = "name";
      Get get = new Get(rowId);
      Result result = hTable.get(get);
      byte[] value = result.getValue(famA, col1);
      System.out.println(Bytes.toString(value));

3.向表中存数据

      下面的例子表示写入一行。行名为abcd，famA列族col1列的数据为"hello world!"。

      byte[] rowId = Bytes.toBytes("abcd");
      byte[] famA = Bytes.toBytes("famA");
      byte[] col1 = Bytes.toBytes("col1");
      Put put = new Put(rowId).
         add(famA, col1, Bytes.toBytes("hello world!"));
      hTable.put(put);
     

4.扫描的用法（scan）：便于获得自己需要的数据，相当于SQL查询。

      byte[] famA = Bytes.toBytes("famA");
      byte[] col1 = Bytes.toBytes("col1");  

      HTable hTable = new HTable("test");  

      //表示要查询的行名是从a开始，到z结束。
      Scan scan = new Scan(Bytes.toBytes("a"), Bytes.toBytes("z"));
     

      //用scan.setStartRow(Bytes.toBytes(""));设置起始行

      //用scan.setStopRow(Bytes.toBytes(""));设置终止行


      //表示查询famA族col1列

      scan.addColumn(famA, col1);  

      //注意，下面是filter的写法。相当于SQL的where子句

      //表示famA族col1列的数据等于"hello world!"
      SingleColumnValueFilter singleColumnValueFilterA = new SingleColumnValueFilter(
           famA, col1, CompareOp.EQUAL, Bytes.toBytes("hello world!"));
      singleColumnValueFilterA.setFilterIfMissing(true);  

      //表示famA族col1列的数据等于"hello hbase!"
      SingleColumnValueFilter singleColumnValueFilterB = new SingleColumnValueFilter(
           famA, col1, CompareOp.EQUAL, Bytes.toBytes("hello hbase!"));
      singleColumnValueFilterB.setFilterIfMissing(true);  
      

      //表示famA族col1列的数据是两者中的一个
      FilterList filter = new FilterList(Operator.MUST_PASS_ONE, Arrays
           .asList((Filter) singleColumnValueFilterA,
                singleColumnValueFilterB));  

      scan.setFilter(filter);  

      ResultScanner scanner = hTable.getScanner(scan);  
      //遍历每个数据
      for (Result result : scanner) {
         System.out.println(Bytes.toString(result.getValue(famA, col1)));
      }

5.上面的代码容易出错的地方在于，需要导入HBase的类所在的包。导入时需要选择包，由于类可能出现在HBase的各个子包中，所以要选择好，下面列出常用的包。尽量用HBase的包

    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.hbase.HBaseConfiguration;
    import org.apache.hadoop.hbase.client.Get;
    import org.apache.hadoop.hbase.client.HTable;
    import org.apache.hadoop.hbase.client.Put;
    import org.apache.hadoop.hbase.client.Result;
    import org.apache.hadoop.hbase.client.ResultScanner;
    import org.apache.hadoop.hbase.client.Scan;
    import org.apache.hadoop.hbase.filter.Filter;
    import org.apache.hadoop.hbase.filter.FilterList;
    import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;
    import org.apache.hadoop.hbase.filter.CompareFilter.CompareOp;
    import org.apache.hadoop.hbase.filter.FilterList.Operator;
    import org.apache.hadoop.hbase.util.Bytes;
    
    import java.io.IOException;
    import java.text.SimpleDateFormat;
    import java.util.Arrays;
    import java.util.Date;


6.下面列出HBase常用的操作

（1）时间戳到时间的转换.单一的时间戳无法给出直观的解释。

    public String GetTimeByStamp(String timestamp)
    {
    
      long datatime= Long.parseLong(timestamp); 
         Date date=new Date(datatime);   
         SimpleDateFormat   format=new   SimpleDateFormat("yyyy-MM-dd HH:MM:ss");   
         String timeresult=format.format(date);
         System.out.println("Time : "+timeresult);
         return timeresult;
    }

（2）时间到时间戳的转换。注意时间是字符串格式。字符串与时间的相互转换，此不赘述。

	public String GetStampByTime(String time)
	{
        String Stamp="";
        SimpleDateFormat sdf=new SimpleDateFormat("yyyy-MM-dd HH:mm:ss");
        Date date;
        try
        {
            date=sdf.parse(time);
            Stamp=date.getTime()+"000";
            System.out.println(Stamp);
        }catch(Exception e){e.printStackTrace();}
        return Stamp;
	}

##架构
HBase主要使用LSM树来提高写性能。


###Memecached
Memcached是一个高性能的分布式内存对象缓存系统，用于动态Web应用以减轻数据库负载。它通过在内存中缓存数据和对象来减少读取数据库的次数，从而提供动态、数据库驱动网站的速度。Memcached基于一个存储键/值对的hashmap。其守护进程(daemon)是用C写的，但是客户端可以用任何语言来编写，并通过memcached协议与守护进程通信。但是它并不提供冗余(例如，复制其hashmap条目)；当某个服务器停止运行或崩溃了，所有的键/值对都将丢失。

以下是memcached在Windows系统下的x86/x64版本的安装方法。

命令提示符下运行(win7下要以管理员身份运行cmd命令)

安装：

	memcached -d install
	memcached -d start
	net start "Memcached Server"

卸载：

	memcached -d stop
	memcached -d remove
	sc delete "Memcached Server"
	
memcached的基本设置：

	-p 监听的端口
	-l 连接的IP地址, 默认是本机
	-d start 启动memcached服务
	-d restart 重起memcached服务
	-d stop|shutdown 关闭正在运行的memcached服务
	-d install 安装memcached服务
	-d uninstall 卸载memcached服务
	-u 以的身份运行 (仅在以root运行的时候有效)
	-m 最大内存使用，单位MB。默认64MB
	-M 内存耗尽时返回错误，而不是删除项
	-c 最大同时连接数，默认是1024
	-f 块大小增长因子，默认是1.25
	-n 最小分配空间，key+value+flags默认是48
	-h 显示帮助

[RegularExpression]: http://www.cnblogs.com/yjhrem/articles/3406149.html/ "regx"

