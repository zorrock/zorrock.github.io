---
layout: post
title: Spark使用笔记
description: Spark在有大量迭代运算情况下，可以直接在内存进行计算，有巨大的性能优势，阔以学习了解下。
category: blog
---

##Scala
Spark是由Scala语言编写的，在Scala语言中函数是一等公民支持简洁的文法来创建函数。并且可重用java库和工具无性能惩罚。
###函数式编程
如果程序中的函数仅接受输入并产生输出，输出只依赖输入，数据不可变，避免保存程序状态，则称为函数式编程，简称FP，又称泛函编程.

* **闭包** 如果一个函数定义在另一个函数的作用域内，并且引用了外层函数的变量，则该函数称为闭包。
##RDD：弹性分布式数据集
RDD是一个有容错机制并可以被并行操作的元素集合。目前有并行集合和hadoop数据集。
###并行集合
通过调用SparkContext的parallelize方法在一个已经存在的Scala集合上创建的一个Seq对象。集合的对象将会被拷贝，创建出一个可以被并行操作的分布式数据集。
一旦分布式数据集被创建好，就可以被并行执行操作。例如可以调用distData(_+_)来将数组元素相加。
并行集合一个重要参数是slices，表示数据集切分的份数。spark将会在集群上为每一份数据起一个任务。例如可以在集群的每个cpu上分布2-4个slices。一般spark会尝试根据集群情况自动设定slices的数目，也可以通过
传递parallelizede的第二个参数来进行手动设置。
###hadoop数据集
##转换
    
* **map** 返回一个新分布式数据集，由每个数据元素经过func函数转换后组成
* **filter** 返回一个新数据集，由经过func函数计算后返回值为true的输入元素组成
* **flatMap** 类似于map，但每个输入元素可以映射为0个或多个输出元素，func返回一个序列而非单一元素
* **mapPartitions** 类似于map,但独立地在RDD的每一个分块上运行，在类型为T的RDD上运行时，func函数必须是Iterator[T]=>Iterator[U]
* **mapPartitionsWithSplit** 类似于mapPartitions，但func带有一个整数参数表示分块的索引值。在类型为T的RDD上运行时，func的函数类型必须是(Int, Iterator[T])=>Iterator[U]
* **sample(withReplacement, fraction, seed)** 根据fraction指定的比例，对数据进行采样，可以选择是否用随机数进行替换，seed用于指定随机数生成器种子
* **union(otherDataset)** 返回一个新的数据集，新数据集是由原数据集和参数指定数据集联合而成
* **distinct([numTasks])** 返回一个包含源数据集中所有不重复元素的新数据集
* **groupByKey([numTasks])** 在一个(K, V)对的数据集上调用，返回一个(K, Seq[V])对的数据集
* **reduceByKey(func,[numTasks])** 在一个(K,V)对的数据集上调用时，返回一个(K,V)对的数据集，使用指定的reduce函数，将相同的key的值聚合到一起。类似groupByKey,reduce任务个数是可以通过第二个可选参数配置
* **sortByKey([ascending, [numTasks])** 在一个(K,V)对的数据集上调用，K必须实现Ordered接口，返回一个按照Key进行排序的(K,V)对数据集。升序或降序由ascending布尔参数决定
* **join(otherDataset, [numTasks])** 在类型为(K,V)和(K,W)类型的数据集上调用时，返回一个相同key对应的所有元素对在一起的(K,(V,W))数据集
* **cogroup(otherDataset, [numTasks])** 在类型为(K,V)和(K,W)的数据集上调用，返回一个(K,Seq[V],Seq[W])元组的数据集。这个操作也称之为groupwith
* **cartesian(otherDataset)** 笛卡尔积，在类型为T和U类型的数据集上调用时，返回一个(T,U)对数据集（两两的元素对)

##动作(actions)
    
* **reduce** 通过函数func(接受两个参数，返回一个参数)聚集数据集中的所有元素。这个功能必须可交换且可关联，从而可以正确的被并行执行。
* **collect** 在驱动程序中，以数组的形式返回数据集的所有元素
* **first** 返回数据集的第一个元素
* **take(n)** 返回一个由数据集的前n个元素组成的数组。这个操作目前由驱动程序计算所有的元素，不是并行执行
* **takeSample(withReplacement, num, seed)** 返回一个数组，在数据集中随机采样num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成种子
* **saveAsTextFile(path)** 将数据集的元素，以textfile的形式，保存到本地文件系统，HDFS或者任何其它hadoop支持的文件系统，对每个元素，Spark将会调用toString方法转化为文件中的文本行
* **saveAsSequenceFile(path)** 将数据集的元素，以hadoop的sequencefile的格式，保存到指定的目录下，本地系统，HDFS或者任何其它hadoop支持的文件系统，这个只限于由key-value对组成，并实现了hadoop的writable接口或可以隐式转换为writable的RDD。
* **countByKey** 对(K,V)类型的RDD有效，返回一个(K,int)对的Map，表示每一个key对应的元素的个数
* **foreach(func)** 在数据集的每一个元素上运行func函数。通常用于边缘效果，例如更新一个累加器，或和外部存储系统进行交互，例如hbase

##实例
###KMeans

    /*
    * K-means clustering.
    * 此例子生成1000个向量，每个向量包含十个数字，任务是要选定10个向量作为质心
    */
    
    objcet KMeans {
        val N = 1000
        val R = 1000 //Scaling factor
        val D = 10
        val K = 10
        val convergeDist = 0.001
        val rand = new Random(42)

        def generateData = {
            def generatePoint(i:Int) = {
                Vector(D, _ => rand.nextDouble*R)
            }
            Array.tabulate(N)(generatePoint)
        }

        def closestPoint(p: Vector, centers: HashMap[Int, Vector]): Int = {
            var index = 0
            var bestIndex = 0
            var closest = Double.PositiveInfinity

            for(i < -1 to centers.size) {
                val vCurr = centers.get(i).get
                val tempDist = p.squaredDist(vCurr)
                if(tempDist < closest) {
                    closest = tempDist
                    bestIndex = i
                }
            }
            bestIndex
        }

        def main(args: Array[String]) {

            val data = generateData
            //初始化数据点，以向量形式存在，包含1000个向量，每个向量包含十个数字点
            var points = new HashSet[Vector] //随机选取的十组向量作为质心
            var kPoints = new HashMap[Int, Vector] //初始质心，共十个，每个质心对应一个向量
            var tempDist = 1.0

            while(points.size < K) {
                points.add(data(rand.nextInt(N)))
            }

            val iter = points.iterator
            for(i <- 1 to points.size) {
                kPoints.put(i, iter.next())
            }
            
            println("Initial centers: " + kPoints)
            
            while(tempDist > convergeDist) {
                //查找每个向量距离最近的质心
                var closest = data.map(p => (closestPoint(p, kPoints), (p, 1)))

                //把具有相同质心的向量聚合在一起
                var mappings = closest.groupBy[Int](x=>x._1)

                //把具有相同质心的向量标量相加
                var pointStats = mappings.map(pair=>pair._2.reduceLeft[(Int,
                (Vector, Int))]{case ((id1, (x1,y1),(id2,(x2,y2)))=>
                (id1, (x1+x2, y1+y2))})

                //找出十个新的质心
                var newPoints = pointStats.map{mapping=>(mapping._1,
                mapping._2._1/mapping._2._2)}

                //迭代收敛条件,即质心变化不大
                tempDist = 0.0
                for(mapping<-newPoints) {
                    tempDist += kPoints.get(mapping._1).get.squaredDist(mapping._2)
                }

                //将新质心放入kPoints
                for(newP<-newPoints) {
                    kPoints.put(newP._1, newP._2)
                }
            }
            println("Final centers: " + kPoints)
        }
    }

###LinearRegresion

    object LinearRegression {
        val sc = new
        SparkContext("local[2]", "BinaryClassification", "user/

        val data = sc.textFile("/user/workspace/data/lpsa.data")

        val parseData = data.map{line=>
            val parts = line.split(',')
            LabeledPoint(parts(0).toDouble, parts(1).split(' ').map(
            x => x.toDouble).toArray)
        }

        //构建模型
        val numIterations = 20
        val model = LineRegressionWithSGD.train(parsedData, numIterations)

        //预测
        val valuesAndPreds = parsedData.map{ point=>
            val prediction = model.predict(point.features)
            (point.label, prediction)
        }

        //计算MSE
        val MSE = valueAndPreds.map{ case(v, p)=>
            math.pow((v-p),2)}.reduce(_+_)/valuesAndPreds.count
            println("training Mean Squared Error = " + MSE)
        }
    }


###CollaborativeFiltering 

    def main(args: Array[String]) {
        val sc = new SparkContext("local[2]","BinaryClassification", "users");
        val data = sc.textFile("/users/test.data")
        //通过数据构造Rating,本质上是一个Tuple[Int, Int, Double]
        val ratings = data.map(_.split(',') match {
            case Array(user, item, rate) => Rating(user.toInt, item.toInt, rate.toDouble)
        }

        //利用ALS构建推荐模型
        val numIterations = 20
        val model = ALS.train(rating, 1, 20, 0.01)

        //预测
        val usersProducts = ratings.map{ case Rating(user, product, rate) =>
            ((user, product), rate)
        }.join(predictions)

        //打印出原始打分及预测打分
        ratesAndPreds.map(x => {
            val user = x._1._1
            val product = x._1._2
            val rate = x._2
            println(s"user, product, rate is $user, $product, $rate")
        }).count

        //计算MSE误差
        val MSE = ratesAndPreds.map{
            case ((user, prduct), (r1, r2)) => math.pow((r1 - r2), 2)
        }.reduce(_+_)/rateAndPreds.count
        println("Mean Squared Error = " + MSE)
        
    }

###PageRank

    //object SparkPageRank {
    //    def showWarning() {
    //        System.err.println(
    //            "hehe")
    //    }

        def main(args:Array[String]) {
            val sparkConf = new SparkConf().setAppName("PageRank")
            val iters = if(args.length>0) args(1).toInt else 10
            val ctx = new SparkContext(sparkConf)
            val lines = ctx.textFile(args(0), 1)
            val links = lines.map{s =>
                val parts = s.split("\\s+")
                (parts(0), parts(1))
            }.distinct().groupByKey().cache()
            var ranks = links.mapValues(v => 1.0)

            for(i <- 1 to iters) {
                val contribs = links.join(ranks).values.flatMap{ case(urls, rank) =>
                    val size = urls.size
                    urls.map(url=>(url, rank/size))
            }
            ranks = contribs.reduceByKey(_+_).mapValues(0.15 + 0.85*_)
        }

        val output = ranks.colllect()
        output.foreach(tup=>println(tup._1 + " has rank: " + tup._2 + "."))

        ctx.stop()

    }


###other

    //wordcount
    val file = spark.textFile("hdfs://...")
    val counts = file.flatMap(line=>line.split(" "))
                     .map(word=>(word, 1))
                     .reduceByKey(_+_)
    counts.saveAsTextFile("hdfs://...")


    //estimating Pi
    val count = spark.parallelize(1 to NUM_SAMPLES).map{i =>
        val x = Math.random()
        val y = Math.random()
        if(x*x + y*y < 1) 1 else 0
    }.reduce(_+_)
    println("Pi is roughly " + 4.0*count/NUM_SAMPLES)

    //logistic regression
    val points = spark.textFile(...).map(parsePoint).cache()
    var w = Vector.random(D) //current separating plane
    for(i <- 1 to ITERATIONS) {
        val gradient = points.map(p =>
            (1/(1 + exp(-p.y*(w dot p.x))) -1) * p.y *p.x
        ).reduce(_+_)
        w-=gradient
    }
    println("Final separating plane: " + w)

    

    


    

