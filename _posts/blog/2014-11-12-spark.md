---
layout: post
title: Spark使用笔记
description: Spark在有大量迭代运算情况下，可以直接在内存进行计算，有巨大的性能优势，阔以学习了解下。
category: blog
---

##Scala
###函数式编程
##RDD：弹性分布式数据集
RDD是一个有容错机制并可以被并行操作的元素集合。目前有并行集合和hadoop数据集。
###并行集合
通过调用SparkContext的parallelize方法在一个已经存在的Scala集合上创建的一个Seq对象。集合的对象将会被拷贝，创建出一个可以被并行操作的分布式数据集。
一旦分布式数据集被创建好，就可以被并行执行操作。例如可以调用distData(_+_)来将数组元素相加。
并行集合一个重要参数是slices，表示数据集切分的份数。spark将会在集群上为每一份数据起一个任务。例如可以在集群的每个cpu上分布2-4个slices。一般spark会尝试根据集群情况自动设定slices的数目，也可以通过
传递parallelizede的第二个参数来进行手动设置。
###hadoop数据集
##转换
    
* **map** 返回一个新分布式数据集，由每个数据元素经过func函数转换后组成
* **filter** 返回一个新数据集，由经过func函数计算后返回值为true的输入元素组成
* **flatMap** 类似于map，但每个输入元素可以映射为0个或多个输出元素，func返回一个序列而非单一元素
* **mapPartitions** 类似于map,但独立地在RDD的每一个分块上运行，在类型为T的RDD上运行时，func函数必须是Iterator[T]=>Iterator[U]
* **mapPartitionsWithSplit** 类似于mapPartitions，但func带有一个整数参数表示分块的索引值。在类型为T的RDD上运行时，func的函数类型必须是(Int, Iterator[T])=>Iterator[U]
* **sample(withReplacement, fraction, seed)** 根据fraction指定的比例，对数据进行采样，可以选择是否用随机数进行替换，seed用于指定随机数生成器种子
* **union(otherDataset)** 返回一个新的数据集，新数据集是由原数据集和参数指定数据集联合而成
* **distinct([numTasks])** 返回一个包含源数据集中所有不重复元素的新数据集
* **groupByKey([numTasks])** 在一个(K, V)对的数据集上调用，返回一个(K, Seq[V])对的数据集
* **reduceByKey(func,[numTasks])** 在一个(K,V)对的数据集上调用时，返回一个(K,V)对的数据集，使用指定的reduce函数，将相同的key的值聚合到一起。类似groupByKey,reduce任务个数是可以通过第二个可选参数配置
* **sortByKey([ascending, [numTasks])** 在一个(K,V)对的数据集上调用，K必须实现Ordered接口，返回一个按照Key进行排序的(K,V)对数据集。升序或降序由ascending布尔参数决定
* **join(otherDataset, [numTasks])** 在类型为(K,V)和(K,W)类型的数据集上调用时，返回一个相同key对应的所有元素对在一起的(K,(V,W))数据集
* **cogroup(otherDataset, [numTasks])** 在类型为(K,V)和(K,W)的数据集上调用，返回一个(K,Seq[V],Seq[W])元组的数据集。这个操作也称之为groupwith
* **cartesian(otherDataset)** 笛卡尔积，在类型为T和U类型的数据集上调用时，返回一个(T,U)对数据集（两两的元素对)

##动作(actions)
    
* **reduce** 通过函数func(接受两个参数，返回一个参数)聚集数据集中的所有元素。这个功能必须可交换且可关联，从而可以正确的被并行执行。
* **collect** 在驱动程序中，以数组的形式返回数据集的所有元素
* **first** 返回数据集的第一个元素
* **take(n)** 返回一个由数据集的前n个元素组成的数组。这个操作目前由驱动程序计算所有的元素，不是并行执行
* **takeSample(withReplacement, num, seed)** 返回一个数组，在数据集中随机采样num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成种子
* **saveAsTextFile(path)** 将数据集的元素，以textfile的形式，保存到本地文件系统，HDFS或者任何其它hadoop支持的文件系统，对每个元素，Spark将会调用toString方法转化为文件中的文本行
* **saveAsSequenceFile(path)** 将数据集的元素，以hadoop的sequencefile的格式，保存到指定的目录下，本地系统，HDFS或者任何其它hadoop支持的文件系统，这个只限于由key-value对组成，并实现了hadoop的writable接口或可以隐式转换为writable的RDD。
* **countByKey** 对(K,V)类型的RDD有效，返回一个(K,int)对的Map，表示每一个key对应的元素的个数
* **foreach(func)** 在数据集的每一个元素上运行func函数。通常用于边缘效果，例如更新一个累加器，或和外部存储系统进行交互，例如hbase

##实例
###KMeans

    /*
    * K-means clustering.
    * 此例子生成1000个向量，每个向量包含十个数字，任务是要选定10个向量作为质心
    */
    
    objcet KMeans {
        val N = 1000
        val R = 1000 //Scaling factor
        val D = 10
        val K = 10
        val convergeDist = 0.001
        val rand = new Random(42)

        def generateData = {
            def generatePoint(i:Int) = {
                Vector(D, _ => rand.nextDouble*R)
            }
            Array.tabulate(N)(generatePoint)
        }

        def closestPoint(p: Vector, centers: HashMap[Int, Vector]): Int = {
            var index = 0
            var bestIndex = 0
            var closest = Double.PositiveInfinity

            for(i < -1 to centers.size) {
                val vCurr = centers.get(i).get
                val tempDist = p.squaredDist(vCurr)
                if(tempDist < closest) {
                    closest = tempDist
                    bestIndex = i
                }
            }
            bestIndex
        }

        def main(args: Array[String]) {

            val data = generateData
            //初始化数据点，以向量形式存在，包含1000个向量，每个向量包含十个数字点
            var points = new HashSet[Vector] //随机选取的十组向量作为质心
            var kPoints = new HashMap[Int, Vector] //初始质心，共十个，每个质心对应一个向量
            var tempDist = 1.0

            while(points.size < K) {
                points.add(data(rand.nextInt(N)))
            }

            val iter = points.iterator
            for(i <- 1 to points.size) {
                kPoints.put(i, iter.next())
            }
            
            println("Initial centers: " + kPoints)
            
            while(tempDist > convergeDist) {
                //查找每个向量距离最近的质心
                var closest = data.map(p => (closestPoint(p, kPoints), (p, 1)))

                //把具有相同质心的向量聚合在一起
                var mappings = closest.groupBy[Int](x=>x._1)

                //把具有相同质心的向量标量相加
                var pointStats = mappings.map(pair=>pair._2.reduceLeft[(Int,
                (Vector, Int))]{case ((id1, (x1,y1),(id2,(x2,y2)))=>
                (id1, (x1+x2, y1+y2))})

                //找出十个新的质心
                var newPoints = pointStats.map{mapping=>(mapping._1,
                mapping._2._1/mapping._2._2)}

                //迭代收敛条件,即质心变化不大
                tempDist = 0.0
                for(mapping<-newPoints) {
                    tempDist += kPoints.get(mapping._1).get.squaredDist(mapping._2)
                }

                //将新质心放入kPoints
                for(newP<-newPoints) {
                    kPoints.put(newP._1, newP._2)
                }
            }

            println("Final centers: " + kPoints)

        }
    }

###LinearRegresion

    object LinearRegression {
        val sc = new
        SparkContext("local[2]", "BinaryClassification", "user/
    }

    


    

